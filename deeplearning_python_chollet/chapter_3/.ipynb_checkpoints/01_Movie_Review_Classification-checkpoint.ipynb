{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS=12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels)  = imdb.load_data(num_words=DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_text(datapoint):\n",
    "    \n",
    "    word_index = imdb.get_word_index()\n",
    "    reversed_word_index = dict([(value, key) for key,value in word_index.items()])\n",
    "    decoded_review = ' '.join([reversed_word_index.get(c-3, '?') for c in datapoint])\n",
    "    return decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_review_text(train_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vectorize_inputs\n",
    "\n",
    "- Accepts data of shape - (num_datapoints, datapoint_length)\n",
    "- Fixes the dimension (here, same as the vocabulary size)\n",
    "- Forms the one-hot encoding for each datapoint\n",
    "\n",
    "\n",
    "'''\n",
    "def vectorize_inputs(sequences, dimensions=DIMENSIONS):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimensions))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorize_inputs(train_data)\n",
    "X_test = vectorize_inputs(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(DIMENSIONS,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 8s 304us/step - loss: 0.4775 - acc: 0.8090 - val_loss: 0.3288 - val_acc: 0.8756\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 6s 240us/step - loss: 0.2345 - acc: 0.9160 - val_loss: 0.2803 - val_acc: 0.8887\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 6s 242us/step - loss: 0.1648 - acc: 0.9430 - val_loss: 0.2933 - val_acc: 0.8846\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 6s 242us/step - loss: 0.1251 - acc: 0.9590 - val_loss: 0.3212 - val_acc: 0.8795\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "hist = model.fit(X_train, y_train, batch_size=512, epochs=20, validation_data=(X_test, y_test), callbacks=[EarlyStopping(mode='min', patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "\n",
    "- You used two hidden layers. Try using one or three hidden layers, and see how doing so affects validation and test accuracy\n",
    "- Try using layers with more hidden units or fewer hidden units: `32` units, `64` units, and so on\n",
    "- Try using the mse loss function instead of binary_crossentropy\n",
    "- Try using the tanh activation (an activation that was popular in the early days of neural networks) instead of `relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
