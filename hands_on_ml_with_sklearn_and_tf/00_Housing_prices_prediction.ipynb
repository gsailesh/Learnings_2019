{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import (\n",
    "    Imputer,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    LabelBinarizer,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fetch_housing_data\n",
    "\n",
    "Fetch the URL - Extract the zipped content\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "load_housing_data\n",
    "\n",
    "Return the CSV as dataframe\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    return pd.read_csv(housing_path + \"/housing.csv\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prepare_test_set\n",
    "\n",
    "Split the dataset into a specific ration\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "np.random.seed(42)\n",
    "def prepare_test_set(data, ratio=0.3):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    training_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "    return data.iloc[training_indices], data.iloc[test_indices]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()\n",
    "housing_data = load_housing_data()\n",
    "\n",
    "housing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_train, housing_data_test = train_test_split(\n",
    "    housing_data, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\n",
    "    \"Training set size: {}, Test set size: {}\".format(\n",
    "        len(housing_data_train), len(housing_data_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A simple `train_test_split()` may not work well for data if there's an unbalanced distribution for any important attribute\n",
    "* In this case, `median_income` is one such important attribute, which has income groups that vary in size\n",
    "\n",
    "* Therefore, `median_income` is divided by 1.5 suitably (looking at the histogram) anda new income category is created for the data, that is more evenly distributed and hence lower sampling bias associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"median_income\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"income_cat\"] = pd.cut(\n",
    "    housing_data[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratSplit = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in stratSplit.split(\n",
    "    housing_data, housing_data[\"income_cat\"]\n",
    "):\n",
    "    housing_data_train = housing_data.iloc[train_index]\n",
    "    housing_data_test = housing_data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    legend=True,\n",
    "    alpha=0.4,\n",
    "    s=housing_data[\"population\"] / 100,\n",
    "    label=\"Population\",\n",
    "    c=\"median_house_value\",\n",
    "    colormap=plt.get_cmap(\"jet\"),\n",
    "    colorbar=True,\n",
    "    title=\"Population Density heatmap\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing_data.corr()\n",
    "\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(\n",
    "    housing_data[\n",
    "        [\"median_house_value\", \"median_income\", \"total_bedrooms\", \"housing_median_age\"]\n",
    "    ],\n",
    "    figsize=(12, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = housing_data_train.drop(\"median_house_value\", axis=1)\n",
    "y_train = housing_data_train[\"median_house_value\"].copy()\n",
    "\n",
    "median_imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "X_train_numeric = X_train_raw.drop(\"ocean_proximity\", axis=1)\n",
    "median_imputer.fit(X_train_numeric)\n",
    "\n",
    "print(\"Median values to be imputed: {}\".format(median_imputer.statistics_))\n",
    "\n",
    "X_train = median_imputer.transform(X_train_numeric)\n",
    "X_train = pd.DataFrame(X_train, columns=X_train_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn’s API is remarkably well designed. The main design principles are:\n",
    "\n",
    "    - **Consistency**. All objects share a consistent and simple interface:\n",
    "    \n",
    "        - **Estimators**. Any object that can estimate some parameters based on a dataset is called an estimator (e.g., an imputer is an estimator). The estimation itself is performed by the `fit()` method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an imputer ’s strategy ), and it must be set as an instance variable (generally via a constructor parameter).\n",
    "        \n",
    "        - **Transformers**. Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the `transform()` method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned parameters, as is the case for an imputer . All transformers also have a convenience method called `fit_transform()` that is equivalent to calling `fit()` and then `transform()` (but sometimes `fit_transform()` is optimized and runs much faster).\n",
    "        \n",
    "        - **Predictors**. Finally, some estimators are capable of making predictions given a dataset; they are called predictors. For example, the `LinearRegression` model in the previous chapter was a predictor: it predicted life satisfaction given a country’s GDP percapita. A predictor has a `predict()` method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a `score()` method that measures the quality of the predictions given a test set (and the corresponding labels in the case of supervised learning algorithms).\n",
    "        \n",
    "    - **Inspection**. All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy ),and all the estimator’s learned parameters are also accessible via public instance variables with an underscore suffix (e.g.,`imputer.statistics_` ).\n",
    "    \n",
    "    - **Nonproliferation of classes**. Datasets are represented as `NumPy` arrays or `SciPy` sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.\n",
    "    \n",
    "    - **Composition**. Existing building blocks are reused as much as possible. For example, it is easy to create a Pipeline estimator from an arbitrary sequence of transformers followed by a final estimator.\n",
    "    \n",
    "    - **Sensible defaults**. Scikit-Learn provides reasonable default values for most parameters, making it easy to create a baseline working system quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Categorical Attributes\n",
    "- LabelEncoder\n",
    "- OneHotEncoder\n",
    "    - OneHot accepts LabelEncoded attribute(s) as input\n",
    "    - OneHot by default returns a Scipy based sparse matrix ; use `.toarray()` to convert it to dense Numpy arrays\n",
    "- LabelBinarizer\n",
    "    - Combines above two and the *binarized* form is returned as dense Numpy by default. By setting `sparse_output=True`, the returned array can be made sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "X_train_cat = X_train_raw[\"ocean_proximity\"]\n",
    "X_train_cat_encoded = encoder.fit_transform(X_train_cat)\n",
    "\n",
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder()\n",
    "X_train_cat_onehot = onehot.fit_transform(X_train_cat_encoded.reshape(-1, 1))\n",
    "\n",
    "X_train_cat_onehot\n",
    "X_train_cat_onehot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = LabelBinarizer()\n",
    "\n",
    "X_train_cat_binarized = binarizer.fit_transform(X_train_cat)\n",
    "\n",
    "X_train_cat_binarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_ids\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "\n",
    "class CombinedHousingAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=False):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        populations_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[\n",
    "                X, rooms_per_household, populations_per_household, bedrooms_per_room\n",
    "            ]\n",
    "\n",
    "        return np.c_[X, rooms_per_household, populations_per_household]\n",
    "\n",
    "\n",
    "attribute_adder = CombinedHousingAttributesAdder(add_bedrooms_per_room=True)\n",
    "\n",
    "X_train_with_extra_attr = attribute_adder.transform(X_train_raw.values)\n",
    "\n",
    "X_train_with_extra_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributeNames):\n",
    "        self.attributeNames = attributeNames\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.attributeNames].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation pipeline (with Feature scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attribs = list(X_train_numeric)\n",
    "categorical_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"selector\", DataFrameSelector(numerical_attribs)),\n",
    "        (\"imputer\", Imputer(strategy=\"median\")),\n",
    "        (\"adder\", CombinedHousingAttributesAdder(add_bedrooms_per_room=True)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"selector\", DataFrameSelector(categorical_attribs)),\n",
    "        (\"one_hot_encoder\", OneHotEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[(\"num_pipeline\", num_pipeline), (\"cat_pipeline\", cat_pipeline)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data_train_X = housing_data_train.drop(columns=[\"median_house_value\"])\n",
    "housing_data_test_X = housing_data_test.drop(columns=[\"median_house_value\"])\n",
    "\n",
    "housing_data_train_Y = housing_data_train[\"median_house_value\"].copy()\n",
    "housing_data_test_Y = housing_data_test[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = full_pipeline.fit_transform(housing_data_train_X)\n",
    "X_test_processed = full_pipeline.fit_transform(housing_data_test_X)\n",
    "\n",
    "model_linear_regression = LinearRegression()\n",
    "model_linear_regression.fit(X_train_processed, housing_data_train_Y)\n",
    "\n",
    "predictions_linear_regression = model_linear_regression.predict(X_test_processed)\n",
    "\n",
    "rmse_linear_regression = np.sqrt(\n",
    "    mean_squared_error(predictions_linear_regression, housing_data_test_Y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor()\n",
    "model_rf.fit(X_train_processed, housing_data_train_Y)\n",
    "\n",
    "predictions_rf = model_rf.predict(X_test_processed)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(predictions_rf, housing_data_test_Y))\n",
    "\n",
    "rf_scores = cross_val_score(\n",
    "    model_rf,\n",
    "    X_train_processed,\n",
    "    housing_data_train_Y,\n",
    "    cv=10,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")\n",
    "# rmse_rf\n",
    "np.sqrt(-rf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter fine-tuning \n",
    "- GridSearchCV\n",
    "- RandomizedSearchCV (for larger search spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\"n_estimators\": [3, 10, 30], \"max_features\": [2, 4, 6, 8]},\n",
    "    {\"n_estimators\": [3, 10], \"max_features\": [2, 3, 4], \"bootstrap\": [False]},\n",
    "]\n",
    "model_rf_cv = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_rf_cv, param_grid, cv=5, scoring=\"neg_mean_squared_error\"\n",
    ")\n",
    "grid_search.fit(X_train_processed, housing_data_train_Y)\n",
    "\n",
    "print(\"Best Params: {}\".format(grid_search.best_params_))\n",
    "print(\"Best Estimator: {}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"n_estimators\": [3, 10, 30],\n",
    "    \"max_features\": [2, 4, 6, 8],\n",
    "    \"bootstrap\": [True, False],\n",
    "}\n",
    "\n",
    "randomized_search = RandomizedSearchCV(\n",
    "    model_rf_cv, param_distributions=param_dist, scoring=\"neg_mean_squared_error\", cv=5\n",
    ")\n",
    "\n",
    "randomized_search.fit(X_train_processed, housing_data_train_Y)\n",
    "\n",
    "print(\"Best Params: {}\".format(randomized_search.best_params_))\n",
    "print(\"Best Estimator: {}\".format(randomized_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid Search based best score: {}\".format(grid_search.best_score_))\n",
    "print(\"Randomized Search based best score: {}\".format(randomized_search.best_score_))\n",
    "\n",
    "grid_rf_rmse = np.sqrt(-grid_search.best_score_)\n",
    "random_rf_rmse = np.sqrt(-randomized_search.best_score_)\n",
    "\n",
    "print(\"Grid Search RMSE: {}\".format(grid_rf_rmse))\n",
    "print(\"Randomized Search RMSE: {}\".format(random_rf_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_rf = randomized_search.best_estimator_\n",
    "final_predictions_rf = final_model_rf.predict(X_test_processed)\n",
    "\n",
    "final_rmse_rf = np.sqrt(mean_squared_error(housing_data_test_Y, final_predictions_rf))\n",
    "print(\"Final RMSE for Random Forest Model: {}\".format(final_rmse_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model_svr = svm.SVR()\n",
    "svr_param_dist = {\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"C\": [0.5, 1],\n",
    "    \"epsilon\": [0.1],\n",
    "}\n",
    "\n",
    "randomized_svr = RandomizedSearchCV(\n",
    "    model_svr,\n",
    "    param_distributions=svr_param_dist,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "randomized_svr.fit(X_train_processed, housing_data_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_svr_rmse = np.sqrt(-randomized_svr.best_score_)\n",
    "print(\"Randomized Search RMSE for SVR: {}\".format(random_svr_rmse))\n",
    "print(\"Randomized SVR best estimator: {}\".format(randomized_svr.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_svr = randomized_svr.best_estimator_\n",
    "final_predictions_svr = final_model_svr.predict(X_test_processed)\n",
    "\n",
    "final_rmse_svr = np.sqrt(mean_squared_error(housing_data_test_Y, final_predictions_svr))\n",
    "print(\"Final RMSE for SVR Model: {}\".format(final_rmse_svr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
