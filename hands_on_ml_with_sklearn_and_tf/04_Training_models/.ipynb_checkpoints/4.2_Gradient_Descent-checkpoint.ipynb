{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent - Intuition\n",
    "\n",
    "- Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what `GradientDescent` does.\n",
    "- It measures the local gradient of the error function with regards to the parameter vector `theta`, and it goes in the direction of descending gradient.\n",
    "\n",
    "\n",
    "- **Note**: Use `StandardScaler` to scale all the features before applying `GradientDescent`. Otherwise, it would take much longer to converge to an optima.\n",
    "- It is a search in the modelâ€™s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is.\n",
    "- Compared to Normal Equation method, `GradientDescent` scales well with number of features.\n",
    "- **Variants** - `Batch` \\& `Stochastic` have different behaviour while converging.\n",
    "    - `Batch` converges slowly and could be trapped in a local minima (in case of non-convex optimization problems) depending on the learning rate.\n",
    "    - `Stochastic` doesn't converge easily and keeps jumping around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent\n",
    "\n",
    "ETA = 0.07\n",
    "N_ITER = 1000\n",
    "m = 100\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "\n",
    "N_EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(m,1)\n",
    "y = 3 + 4*X + np.random.rand(m,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((m,1)), X]\n",
    "X_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.3790391 ],\n",
       "       [ 0.53659451]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_random = np.random.randn(2,1) # random_initialization\n",
    "theta_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.54883567],\n",
       "       [3.939423  ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(N_ITER):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta_random) - y)\n",
    "    theta_random -= ETA * gradients\n",
    "    \n",
    "theta_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(t, t0=5, t1=50):\n",
    "    return t0/(t+t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05019794],\n",
       "       [-0.7452785 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_init = np.random.randn(2,1) # random initialization\n",
    "theta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.55351015],\n",
       "       [3.93941958]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    for i in range(m):\n",
    "        \n",
    "        random_idx = np.random.randint(m)\n",
    "        \n",
    "        X_i = X_b[random_idx:random_idx+1]\n",
    "        y_i = y[random_idx:random_idx+1]\n",
    "        \n",
    "        gradients = 2 * X_i.T.dot(X_i.dot(theta_init) - y_i)\n",
    "        \n",
    "        eta = learning_schedule(epoch*m + i)\n",
    "        \n",
    "        theta_init -= eta*gradients\n",
    "        \n",
    "        \n",
    "theta_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "- Above implementation randomly picks up observations from the dataset to calculate individual gradients. However, this could lead to repetitive choice of observations.\n",
    "- Below implementation from `sklearn` - `SGDRegressor` performs Linear Regression using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [3.55020373], Coefficient: [3.94151513]\n"
     ]
    }
   ],
   "source": [
    "sgd_regressor = SGDRegressor(n_iter=100, penalty=None, eta0=0.07)\n",
    "sgd_regressor.fit(X, y.ravel())\n",
    "\n",
    "print(\"Intercept: {}, Coefficient: {}\".format(sgd_regressor.intercept_, sgd_regressor.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm | Large m | Large n | Hyperparams | \n",
    "--- | --- | --- | --- |\n",
    "Normal Equation | Fast | Slow | None |\n",
    "Batch GD | Slow | Fast | 2 |\n",
    "Stochastic GD | Fast | Fast | >=2 |\n",
    "Mini-batch GD | Fast | Fast | >=2 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
